{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d00737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0874f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from this import d\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05909c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Reacher-v2')\n",
    "test_env = gym.make('Reacher-v2')\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device = ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a57f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "class MLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit).to(device)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f6e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('obs', 'act', 'rew', 'next_obs', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef79a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim =  11\n",
      "act_dim =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1540/1024134745.py:140: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  o = torch.tensor([o], device=device)\n",
      "/tmp/ipykernel_1540/1024134745.py:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = ac.act(torch.tensor(o, dtype=torch.float32, device=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  1000\n",
      "main od rewards =  -0.39225072074416983\n",
      "t =  1050\n",
      "main od rewards =  -0.4189915411928245\n",
      "t =  1100\n",
      "main od rewards =  -0.3316896402893972\n",
      "t =  1150\n",
      "main od rewards =  -0.619270515888225\n",
      "t =  1200\n",
      "main od rewards =  -1.0872440833218924\n",
      "t =  1250\n",
      "main od rewards =  -1.0428896960137006\n",
      "t =  1300\n",
      "main od rewards =  -1.1284602238202552\n",
      "t =  1350\n",
      "main od rewards =  -1.0666593072279789\n",
      "t =  1400\n",
      "main od rewards =  -1.2504264116329733\n",
      "t =  1450\n",
      "main od rewards =  -1.2069146208856154\n",
      "t =  1500\n",
      "main od rewards =  -1.592110906673233\n",
      "t =  1550\n",
      "main od rewards =  -1.886543932385081\n",
      "t =  1600\n",
      "main od rewards =  -1.8076372298501318\n",
      "t =  1650\n",
      "main od rewards =  -1.9317593043480152\n",
      "t =  1700\n",
      "main od rewards =  -1.9955999531008408\n",
      "t =  1750\n",
      "main od rewards =  -1.8474744672041892\n",
      "t =  1800\n",
      "main od rewards =  -1.854689505652942\n",
      "t =  1850\n",
      "main od rewards =  -2.0243859194717717\n",
      "t =  1900\n",
      "main od rewards =  -1.984808734219329\n",
      "t =  1950\n",
      "main od rewards =  -2.0214688553435805\n",
      "t =  2000\n",
      "main od rewards =  -1.8554531003066648\n",
      "t =  2050\n",
      "main od rewards =  -1.834177978067513\n",
      "t =  2100\n",
      "main od rewards =  -1.787181242531244\n",
      "t =  2150\n",
      "main od rewards =  -2.113665102075618\n",
      "t =  2200\n",
      "main od rewards =  -2.0979280670193154\n",
      "t =  2250\n",
      "main od rewards =  -2.1680154153559035\n",
      "t =  2300\n",
      "main od rewards =  -2.1405631022754186\n",
      "t =  2350\n",
      "main od rewards =  -2.0550785765196244\n",
      "t =  2400\n",
      "main od rewards =  -2.031811681030824\n",
      "t =  2450\n",
      "main od rewards =  -2.032939792393031\n",
      "t =  2500\n",
      "main od rewards =  -2.00591762805359\n",
      "t =  2550\n",
      "main od rewards =  -1.95374798792602\n",
      "t =  2600\n",
      "main od rewards =  -1.9518302393618985\n",
      "t =  2650\n",
      "main od rewards =  -1.8707206951896278\n",
      "t =  2700\n",
      "main od rewards =  -1.806297936535222\n",
      "t =  2750\n",
      "main od rewards =  -1.711228697788186\n",
      "t =  2800\n",
      "main od rewards =  -1.846534222419299\n",
      "t =  2850\n",
      "main od rewards =  -2.107323801713929\n",
      "t =  2900\n",
      "main od rewards =  -2.162468606027168\n",
      "t =  2950\n",
      "main od rewards =  -2.1663345699081455\n",
      "t =  3000\n",
      "main od rewards =  -2.088823548671453\n",
      "t =  3050\n",
      "main od rewards =  -2.195958433832843\n",
      "t =  3100\n",
      "main od rewards =  -2.1367524731124177\n",
      "t =  3150\n",
      "main od rewards =  -2.0388774361289634\n",
      "t =  3200\n",
      "main od rewards =  -2.194903607436276\n",
      "t =  3250\n",
      "main od rewards =  -2.1663528364615656\n",
      "t =  3300\n",
      "main od rewards =  -2.113600241762286\n",
      "t =  3350\n",
      "main od rewards =  -2.145235171064502\n",
      "t =  3400\n",
      "main od rewards =  -2.1252669768150017\n",
      "t =  3450\n",
      "main od rewards =  -2.228895362238772\n",
      "t =  3500\n",
      "main od rewards =  -2.0781592590815654\n",
      "t =  3550\n",
      "main od rewards =  -2.0809106342856216\n",
      "t =  3600\n",
      "main od rewards =  -2.034636676658893\n",
      "t =  3650\n",
      "main od rewards =  -2.0846718995445968\n",
      "t =  3700\n",
      "main od rewards =  -1.9607177020828017\n",
      "t =  3750\n",
      "main od rewards =  -2.0774214124186066\n",
      "t =  3800\n",
      "main od rewards =  -2.0515877768642796\n",
      "t =  3850\n",
      "main od rewards =  -2.087473167733449\n",
      "t =  3900\n",
      "main od rewards =  -2.1409331329447343\n",
      "t =  3950\n",
      "main od rewards =  -2.025152487781334\n",
      "t =  4000\n",
      "main od rewards =  -2.204993977057163\n",
      "t =  4050\n",
      "main od rewards =  -2.130119456959753\n",
      "t =  4100\n",
      "main od rewards =  -2.1711954175891894\n",
      "t =  4150\n",
      "main od rewards =  -2.049262355129482\n",
      "t =  4200\n",
      "main od rewards =  -2.048720223153904\n",
      "t =  4250\n",
      "main od rewards =  -2.116298345479417\n",
      "t =  4300\n",
      "main od rewards =  -2.0852011469287834\n",
      "t =  4350\n",
      "main od rewards =  -2.063070930217479\n",
      "t =  4400\n",
      "main od rewards =  -2.258890416044794\n",
      "t =  4450\n",
      "main od rewards =  -1.9893195934865786\n",
      "t =  4500\n",
      "main od rewards =  -2.2728727087335034\n",
      "t =  4550\n",
      "main od rewards =  -2.2106379684817963\n",
      "t =  4600\n",
      "main od rewards =  -2.2773992825096903\n",
      "t =  4650\n",
      "main od rewards =  -2.230407722856473\n",
      "t =  4700\n",
      "main od rewards =  -2.2022652242879235\n",
      "t =  4750\n",
      "main od rewards =  -2.2157867465742\n",
      "t =  4800\n",
      "main od rewards =  -2.295221931497586\n",
      "t =  4850\n",
      "main od rewards =  -2.203587536654337\n",
      "t =  4900\n",
      "main od rewards =  -2.202307357020658\n",
      "t =  4950\n",
      "main od rewards =  -2.106487866541812\n",
      "t =  5000\n",
      "main od rewards =  -2.2736779639464717\n",
      "t =  5050\n",
      "main od rewards =  -2.001082353963353\n",
      "t =  5100\n",
      "main od rewards =  -2.184811692315064\n",
      "t =  5150\n",
      "main od rewards =  -2.0451689577074843\n",
      "t =  5200\n",
      "main od rewards =  -1.9886905886921373\n",
      "t =  5250\n",
      "main od rewards =  -2.173244043609374\n",
      "t =  5300\n",
      "main od rewards =  -2.0944349866726286\n",
      "t =  5350\n",
      "main od rewards =  -2.228855352926423\n",
      "t =  5400\n",
      "main od rewards =  -2.187750360643942\n",
      "t =  5450\n",
      "main od rewards =  -2.3111097953011317\n",
      "t =  5500\n",
      "main od rewards =  -2.1082840838567276\n",
      "t =  5550\n",
      "main od rewards =  -2.178911998460701\n",
      "t =  5600\n",
      "main od rewards =  -2.2070450766562644\n",
      "t =  5650\n",
      "main od rewards =  -2.246742999703891\n",
      "t =  5700\n",
      "main od rewards =  -2.2789320522441017\n",
      "t =  5750\n",
      "main od rewards =  -2.1154107342716753\n",
      "t =  5800\n",
      "main od rewards =  -2.1053693832006357\n",
      "t =  5850\n",
      "main od rewards =  -2.2791391916033303\n",
      "t =  5900\n",
      "main od rewards =  -2.2978582422160008\n",
      "t =  5950\n",
      "main od rewards =  -2.2663514963084075\n",
      "t =  6000\n",
      "main od rewards =  -2.1999364783577118\n",
      "t =  6050\n",
      "main od rewards =  -2.163219158480942\n",
      "t =  6100\n",
      "main od rewards =  -2.330229202423933\n",
      "t =  6150\n",
      "main od rewards =  -2.3429104762957107\n",
      "t =  6200\n",
      "main od rewards =  -2.3472472167975966\n",
      "t =  6250\n",
      "main od rewards =  -2.145743408976185\n",
      "t =  6300\n",
      "main od rewards =  -2.2827679972516304\n",
      "t =  6350\n",
      "main od rewards =  -2.2459071898036207\n",
      "t =  6400\n",
      "main od rewards =  -2.259373559598255\n",
      "t =  6450\n",
      "main od rewards =  -2.1842507632042953\n",
      "t =  6500\n",
      "main od rewards =  -2.3022714789009084\n",
      "t =  6550\n",
      "main od rewards =  -2.2616550221005083\n",
      "t =  6600\n",
      "main od rewards =  -2.222975505284071\n",
      "t =  6650\n",
      "main od rewards =  -2.0284113587074897\n",
      "t =  6700\n",
      "main od rewards =  -2.217506490862585\n",
      "t =  6750\n",
      "main od rewards =  -2.3050635834428643\n",
      "t =  6800\n",
      "main od rewards =  -2.055939664524782\n",
      "t =  6850\n",
      "main od rewards =  -2.1824885475502653\n",
      "t =  6900\n",
      "main od rewards =  -2.2396136429080267\n",
      "t =  6950\n",
      "main od rewards =  -2.1564961927771313\n",
      "t =  7000\n",
      "main od rewards =  -2.2242790151740515\n",
      "t =  7050\n",
      "main od rewards =  -2.3407701675672032\n",
      "t =  7100\n",
      "main od rewards =  -2.061463984349211\n",
      "t =  7150\n",
      "main od rewards =  -2.3333582618418705\n",
      "t =  7200\n",
      "main od rewards =  -2.2451891171639646\n",
      "t =  7250\n",
      "main od rewards =  -2.1720260454647446\n",
      "t =  7300\n",
      "main od rewards =  -2.1856355839363877\n",
      "t =  7350\n",
      "main od rewards =  -2.248881956150119\n",
      "t =  7400\n",
      "main od rewards =  -2.2943252759732875\n",
      "t =  7450\n",
      "main od rewards =  -2.127068753162688\n",
      "t =  7500\n",
      "main od rewards =  -2.305931706476283\n",
      "t =  7550\n",
      "main od rewards =  -2.1903931295784282\n",
      "t =  7600\n",
      "main od rewards =  -2.3360887236164123\n",
      "t =  7650\n",
      "main od rewards =  -2.338104433313463\n",
      "t =  7700\n",
      "main od rewards =  -2.081861209250405\n",
      "t =  7750\n",
      "main od rewards =  -2.22529314695052\n",
      "t =  7800\n",
      "main od rewards =  -2.3247778317291634\n",
      "t =  7850\n",
      "main od rewards =  -2.1312588818392983\n",
      "t =  7900\n",
      "main od rewards =  -2.3282267673568646\n",
      "t =  7950\n",
      "main od rewards =  -2.2543960893260753\n",
      "t =  8000\n",
      "main od rewards =  -2.0270146917226577\n",
      "t =  8050\n",
      "main od rewards =  -2.025815502534402\n",
      "t =  8100\n",
      "main od rewards =  -2.1425243039722095\n",
      "t =  8150\n",
      "main od rewards =  -2.1305211336970022\n",
      "t =  8200\n",
      "main od rewards =  -2.1270844955280936\n",
      "t =  8250\n",
      "main od rewards =  -2.289455381313642\n",
      "t =  8300\n",
      "main od rewards =  -2.347375807819853\n",
      "t =  8350\n",
      "main od rewards =  -2.2232461710398512\n",
      "t =  8400\n",
      "main od rewards =  -2.234449350665149\n",
      "t =  8450\n",
      "main od rewards =  -2.127002166984154\n",
      "t =  8500\n",
      "main od rewards =  -2.2333100957157086\n",
      "t =  8550\n",
      "main od rewards =  -2.092100730763772\n",
      "t =  8600\n",
      "main od rewards =  -2.2481984768925303\n",
      "t =  8650\n",
      "main od rewards =  -2.2806304228290006\n",
      "t =  8700\n",
      "main od rewards =  -2.180683168547852\n",
      "t =  8750\n",
      "main od rewards =  -2.1144253599699523\n",
      "t =  8800\n",
      "main od rewards =  -2.3739388842582754\n",
      "t =  8850\n",
      "main od rewards =  -2.203125104712952\n",
      "t =  8900\n",
      "main od rewards =  -2.2327290007690097\n",
      "t =  8950\n",
      "main od rewards =  -2.1656035481762004\n",
      "t =  9000\n",
      "main od rewards =  -2.103311335341965\n",
      "t =  9050\n",
      "main od rewards =  -2.259126205576262\n",
      "t =  9100\n",
      "main od rewards =  -2.0431068016637357\n",
      "t =  9150\n",
      "main od rewards =  -2.2188318725698113\n",
      "t =  9200\n",
      "main od rewards =  -2.36988658769926\n",
      "t =  9250\n",
      "main od rewards =  -2.138579373398506\n",
      "t =  9300\n",
      "main od rewards =  -2.3176370601975114\n",
      "t =  9350\n",
      "main od rewards =  -2.2399073709619346\n",
      "t =  9400\n",
      "main od rewards =  -2.0973373825379102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  9450\n",
      "main od rewards =  -2.185223021064987\n",
      "t =  9500\n",
      "main od rewards =  -2.2461417988814483\n",
      "t =  9550\n",
      "main od rewards =  -2.1498866943644144\n",
      "t =  9600\n",
      "main od rewards =  -2.2866247083911326\n",
      "t =  9650\n",
      "main od rewards =  -2.2729645511613006\n",
      "t =  9700\n",
      "main od rewards =  -2.1366899463114835\n",
      "t =  9750\n",
      "main od rewards =  -2.2506834630605344\n",
      "t =  9800\n",
      "main od rewards =  -2.139110674413405\n",
      "t =  9850\n",
      "main od rewards =  -2.2895278979380764\n",
      "t =  9900\n",
      "main od rewards =  -2.2722197680447787\n",
      "t =  9950\n",
      "main od rewards =  -2.2824051193186143\n",
      "t =  10000\n",
      "main od rewards =  -2.3059103210444674\n",
      "t =  10050\n",
      "main od rewards =  -2.236121005770428\n",
      "t =  10100\n",
      "main od rewards =  -2.1399204947160566\n",
      "t =  10150\n",
      "main od rewards =  -2.176090521099632\n",
      "t =  10200\n",
      "main od rewards =  -2.193799999767783\n",
      "t =  10250\n",
      "main od rewards =  -2.25189131448729\n",
      "t =  10300\n",
      "main od rewards =  -2.10549664988322\n",
      "t =  10350\n",
      "main od rewards =  -2.0922338966611917\n",
      "t =  10400\n",
      "main od rewards =  -2.245843153999257\n",
      "t =  10450\n",
      "main od rewards =  -2.26280263153197\n",
      "t =  10500\n",
      "main od rewards =  -2.3488956045377454\n",
      "t =  10550\n",
      "main od rewards =  -2.190295574416753\n",
      "t =  10600\n",
      "main od rewards =  -2.210984265960042\n",
      "t =  10650\n",
      "main od rewards =  -2.174943469803192\n",
      "t =  10700\n",
      "main od rewards =  -2.24786853654668\n",
      "t =  10750\n",
      "main od rewards =  -2.040014067480602\n",
      "t =  10800\n",
      "main od rewards =  -2.301426510944771\n",
      "t =  10850\n",
      "main od rewards =  -2.2162044088390425\n",
      "t =  10900\n",
      "main od rewards =  -2.1286372043261625\n",
      "t =  10950\n",
      "main od rewards =  -2.11488359816344\n",
      "t =  11000\n",
      "main od rewards =  -2.1506635675974386\n",
      "t =  11050\n",
      "main od rewards =  -2.2863022671451563\n",
      "t =  11100\n",
      "main od rewards =  -2.279891637537089\n",
      "t =  11150\n",
      "main od rewards =  -2.2792620499656047\n",
      "t =  11200\n",
      "main od rewards =  -2.29680038776415\n",
      "t =  11250\n",
      "main od rewards =  -2.2337450533366803\n",
      "t =  11300\n",
      "main od rewards =  -2.0714345478958442\n",
      "t =  11350\n",
      "main od rewards =  -2.2582823121814\n",
      "t =  11400\n",
      "main od rewards =  -2.1976833623282275\n",
      "t =  11450\n",
      "main od rewards =  -2.15427335488292\n",
      "t =  11500\n",
      "main od rewards =  -2.163466163082556\n",
      "t =  11550\n",
      "main od rewards =  -2.2189401358643805\n",
      "t =  11600\n",
      "main od rewards =  -2.3277864930272143\n",
      "t =  11650\n",
      "main od rewards =  -2.004778291133195\n",
      "t =  11700\n",
      "main od rewards =  -2.021965640717508\n",
      "t =  11750\n",
      "main od rewards =  -2.2387123490458154\n",
      "t =  11800\n",
      "main od rewards =  -2.2572737169326076\n",
      "t =  11850\n",
      "main od rewards =  -2.1211023675123943\n",
      "t =  11900\n",
      "main od rewards =  -2.0530571339123727\n",
      "t =  11950\n",
      "main od rewards =  -2.1819840335425704\n",
      "t =  12000\n",
      "main od rewards =  -2.179769515288515\n",
      "t =  12050\n",
      "main od rewards =  -2.271111402842278\n",
      "t =  12100\n",
      "main od rewards =  -2.1337576631508703\n",
      "t =  12150\n",
      "main od rewards =  -2.2747413551254847\n",
      "t =  12200\n",
      "main od rewards =  -2.191429658935752\n",
      "t =  12250\n",
      "main od rewards =  -2.2088269207903375\n",
      "t =  12300\n",
      "main od rewards =  -2.1538052826152385\n",
      "t =  12350\n",
      "main od rewards =  -2.135229144368267\n",
      "t =  12400\n",
      "main od rewards =  -2.0677293640756953\n",
      "t =  12450\n",
      "main od rewards =  -2.04813968440794\n",
      "t =  12500\n",
      "main od rewards =  -1.9551079752154263\n",
      "t =  12550\n",
      "main od rewards =  -1.2573349797877822\n",
      "t =  12600\n",
      "main od rewards =  -1.3096748555509494\n",
      "t =  12650\n",
      "main od rewards =  -1.5528326423424799\n",
      "t =  12700\n",
      "main od rewards =  -0.9373406323513328\n",
      "t =  12750\n",
      "main od rewards =  -2.228433896528852\n",
      "t =  12800\n",
      "main od rewards =  -2.0800661637474054\n",
      "t =  12850\n",
      "main od rewards =  -1.8500941882071622\n",
      "t =  12900\n",
      "main od rewards =  -2.207391573945235\n",
      "t =  12950\n",
      "main od rewards =  -2.00117221254372\n",
      "t =  13000\n",
      "main od rewards =  -1.5778452097235558\n",
      "t =  13050\n",
      "main od rewards =  -2.208551885504995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1540/1024134745.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# print('batch = ', str(batch))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# End of epoch handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1540/1024134745.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mloss_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mloss_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mq_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cosmic_rays_x_py38_env/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cosmic_rays_x_py38_env/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hid = 256\n",
    "l = 2\n",
    "\n",
    "\n",
    "ac_kwargs=dict(hidden_sizes=[hid]*l)\n",
    "seed=0\n",
    "steps_per_epoch=3000\n",
    "epochs=100\n",
    "replay_size=int(1e6)\n",
    "gamma=0.99\n",
    "polyak=0.995\n",
    "pi_lr=1e-4\n",
    "q_lr=1e-4\n",
    "batch_size=500\n",
    "start_steps=10000 \n",
    "update_after=1000\n",
    "update_every=50\n",
    "act_noise=0.01\n",
    "num_test_episodes=10\n",
    "max_ep_len=1000\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "print('obs_dim = ', obs_dim)\n",
    "act_dim = env.action_space.shape[0]\n",
    "print('act_dim = ', act_dim)\n",
    "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "act_limit = env.action_space.high[0]\n",
    "\n",
    "# Create actor-critic module and target networks\n",
    "ac = MLPActorCritic(env.observation_space, env.action_space, **ac_kwargs)\n",
    "ac_targ = deepcopy(ac)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in ac_targ.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "memory = ReplayMemory(replay_size)\n",
    "\n",
    "# Count variables (protip: try to get a feel for how different size networks behave!)\n",
    "var_counts = tuple(count_vars(module) for module in [ac.pi, ac.q])\n",
    "\n",
    "# Set up function for computing DDPG Q-loss\n",
    "def compute_loss_q(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "    a = torch.cat(data.act).float()\n",
    "    r = torch.cat(data.rew).float()\n",
    "    o2 =torch.cat(data.next_obs).float()\n",
    "    d = torch.cat(data.done).float()\n",
    "\n",
    "    q = ac.q(o,a)\n",
    "\n",
    "\n",
    "    # Bellman backup for Q function\n",
    "    with torch.no_grad():\n",
    "        q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "        backup = r + gamma * (1 - d) * q_pi_targ\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q = ((q - backup)**2).mean()\n",
    "\n",
    "    return loss_q\n",
    "\n",
    "# Set up function for computing DDPG pi loss\n",
    "def compute_loss_pi(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "\n",
    "    q_pi = ac.q(o, ac.pi(o))\n",
    "\n",
    "    return -q_pi.mean()\n",
    "\n",
    "pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "q_optimizer = Adam(ac.q.parameters(), lr=q_lr)\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "\n",
    "\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q = compute_loss_q(data)\n",
    "\n",
    "    loss_q.backward()\n",
    "\n",
    "    q_optimizer.step()\n",
    "\n",
    "\n",
    "    # Freeze Q-network so you don't waste computational effort \n",
    "    # computing gradients for it during the policy learning step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Next run one gradient descent step for pi.\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(polyak)\n",
    "            p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "\n",
    "def get_action(o, noise_scale):\n",
    "    a = ac.act(torch.tensor(o, dtype=torch.float32, device=device))\n",
    "    a += noise_scale * torch.randn(act_dim).to(device)\n",
    "    return torch.clip(a, -act_limit, act_limit)\n",
    "\n",
    "def test_agent():\n",
    "    for j in range(num_test_episodes):\n",
    "        o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "        while not(d or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "            o, r, d, _ = test_env.step(a_cpu)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "total_steps = steps_per_epoch * epochs\n",
    "start_time = time.time()\n",
    "o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for t in range(total_steps):\n",
    "\n",
    "\n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    if t > start_steps:\n",
    "        a = get_action(o, act_noise)\n",
    "    else:\n",
    "        a = env.action_space.sample()\n",
    "        a = torch.tensor([a], device=device)\n",
    "    a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "    # Step the env\n",
    "    o2, r, d, _ = env.step(a_cpu)\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "    ep_ret_main = ep_ret/ep_len\n",
    "\n",
    "\n",
    "    # Ignore the \"done\" signal if it comes from hitting the time\n",
    "    # horizon (that is, when it's an artificial terminal signal\n",
    "    # that isn't based on the agent's state)\n",
    "    d = False if ep_len==max_ep_len else d\n",
    "\n",
    "    # a = torch.tensor([a], device=device)\n",
    "    o2 = torch.tensor([o2], device=device)\n",
    "    r = torch.tensor([r], device=device)\n",
    "    d = torch.tensor([d], device=device)\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    memory.push(o, a, r, o2, d)\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o=o2\n",
    "\n",
    "    # End of trajectory handling\n",
    "    if d or (ep_len == max_ep_len):\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "    # Update handling\n",
    "    if t >= update_after and t % update_every == 0:\n",
    "        print('t = ', t)\n",
    "        print('main od rewards = ', ep_ret_main)\n",
    "        for i in range(update_every):\n",
    "\n",
    "            transitions = memory.sample(batch_size)\n",
    "            # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "            # detailed explanation). This converts batch-array of Transitions\n",
    "            # to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            # print('batch = ', str(batch))\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % steps_per_epoch == 0:\n",
    "        epoch = (t+1) // steps_per_epoch\n",
    "\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('cosmic_rays_x_py38_env': conda)",
   "language": "python",
   "name": "python3812jvsc74a57bd01a48a149bc7a8dee0435672efcae6f64e48d62311a35302b209b3ac517d7f9c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d00737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"/home/xhnfly/Cosmic_rays_X/XRL/\")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10c563",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0874f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from this import d\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop\n",
    "import XRL_main\n",
    "import gym\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "import neptune.new as neptune\n",
    "import datetime\n",
    "\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_log = neptune.init(\n",
    "    project=\"xhnfirst/DDPG-mian-test1\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NTg5MDI2OS01MTVmLTQ2YjUtODA1Yy02ZWQyNDgxZDcwN2UifQ==\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32990c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "from time import strftime\n",
    "from time import gmtime\n",
    "\n",
    "class view_bar:\n",
    "    def __init__(self, message, num, total, value_name, value_data, start_time_rec, r_true):\n",
    "        self.message = message\n",
    "        self.num = num\n",
    "        self.total = total\n",
    "        self.r_true = r_true\n",
    "        self.message2 = \"True time of r = \"\n",
    "        self.message3 = \"time cost = \"\n",
    "        rate = self.num  / self.total\n",
    "        rate_num = int(rate * 40)\n",
    "        rate_nums = math.ceil(rate * 100)\n",
    "        time_rec = datetime.datetime.now()\n",
    "        execution_time = time_rec - start_time_rec\n",
    "\n",
    "\n",
    "        r = '\\r%s:[%s%s]%d%%\\t%d/%d\\t%s%f\\t%s%s\\t%s%s' % (self.message, \"=\" * rate_num,\n",
    "                                        \" \" * (40 - rate_num), rate_nums, self.num , self.total, value_name, value_data, self.message3, execution_time, self.message2, str(self.r_true))\n",
    "\n",
    "        sys.stdout.write(r)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "# a =10\n",
    "# for i in range(60000):\n",
    "#     view_bar(\"epoch \", i+1, 60000, 'a = ', float(a))\n",
    "#     a += 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05909c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Reacher-v2')\n",
    "test_env = gym.make('Reacher-v2')\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device = ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a57f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit).to(device)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('obs', 'act', 'rew', 'next_obs', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef79a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r =  -1.668342245574622\n",
      "d =  False\n",
      "r =  -1.8526876968828192\n",
      "d =  False\n",
      "r =  -1.6597030317270076\n",
      "d =  False\n",
      "r =  -1.5049055332145436\n",
      "d =  False\n",
      "r =  -1.6089879745124633\n",
      "d =  False\n",
      "r =  -1.597133125252244\n",
      "d =  False\n",
      "r =  -1.4091722368505915\n",
      "d =  False\n",
      "r =  -1.1871197735891004\n",
      "d =  False\n",
      "r =  -1.118426095021267\n",
      "d =  False\n",
      "r =  -1.1456230932961722\n",
      "d =  False\n",
      "r =  -1.072164134394387\n",
      "d =  False\n",
      "r =  -1.2064633221464078\n",
      "d =  False\n",
      "r =  -0.5984520495110661\n",
      "d =  False\n",
      "r =  -0.717556463480573\n",
      "d =  False\n",
      "r =  -0.5356604237995711\n",
      "d =  False\n",
      "r =  -0.4008689891222197\n",
      "d =  False\n",
      "r =  -0.5561161899156417\n",
      "d =  False\n",
      "r =  -0.864772222488631\n",
      "d =  False\n",
      "r =  -0.9270559318115437\n",
      "d =  False\n",
      "r =  -0.7695818091938607\n",
      "d =  False\n",
      "r =  -0.5316400806323962\n",
      "d =  False\n",
      "r =  -0.3975056154483827\n",
      "d =  False\n",
      "r =  -0.7166902857770469\n",
      "d =  False\n",
      "r =  -0.9134690231433539\n",
      "d =  False\n",
      "r =  -0.8036148134971237\n",
      "d =  False\n",
      "r =  -0.5127427644185356\n",
      "d =  False\n",
      "r =  -0.38565974180064044\n",
      "d =  False\n",
      "r =  -0.7319629256977036\n",
      "d =  False\n",
      "r =  -0.8661440216966407\n",
      "d =  False\n",
      "r =  -0.6389978969903324\n",
      "d =  False\n",
      "r =  -0.42751777993010365\n",
      "d =  False\n",
      "r =  -0.5986958514182164\n",
      "d =  False\n",
      "r =  -0.87691282356115\n",
      "d =  False\n",
      "r =  -0.8026096901101825\n",
      "d =  False\n",
      "r =  -0.5116469796528816\n",
      "d =  False\n",
      "r =  -0.5638571115522821\n",
      "d =  False\n",
      "r =  -0.8995773883496682\n",
      "d =  False\n",
      "r =  -0.8383019474126452\n",
      "d =  False\n",
      "r =  -0.5339291918782475\n",
      "d =  False\n",
      "r =  -0.6133303633325294\n",
      "d =  False\n",
      "r =  -0.9000359268110678\n",
      "d =  False\n",
      "r =  -0.7481609796883613\n",
      "d =  False\n",
      "r =  -0.49332940252460233\n",
      "d =  False\n",
      "r =  -0.7818881547159718\n",
      "d =  False\n",
      "r =  -0.9642676133469334\n",
      "d =  False\n",
      "r =  -0.6121968062882966\n",
      "d =  False\n",
      "r =  -0.6326408899014176\n",
      "d =  False\n",
      "r =  -0.9810909320884037\n",
      "d =  False\n",
      "r =  -0.7568204724227013\n",
      "d =  True\n",
      "r =  -0.8422685040116511\n",
      "d =  False\n",
      "steps: :[===                                     ]8%\t23750/300000\tmain of rewards = -1.842269\ttime cost = 0:02:54.763711\tTrue time of r = 475r =  -1.577583368692706\n",
      "d =  False\n",
      "r =  -1.7707107392564654\n",
      "d =  False\n",
      "r =  -1.545553630768191\n",
      "d =  False\n",
      "r =  -1.348775044501327\n",
      "d =  False\n",
      "r =  -1.4457847740220626\n",
      "d =  False\n",
      "r =  -1.4257613369932134\n",
      "d =  False\n",
      "r =  -1.3163767846784382\n",
      "d =  False\n",
      "r =  -1.2113544240620675\n",
      "d =  False\n",
      "r =  -1.1605593404350492\n",
      "d =  False\n",
      "r =  -1.31097239373712\n",
      "d =  False\n",
      "r =  -1.0649187043680997\n",
      "d =  False\n",
      "r =  -1.0335608124115294\n",
      "d =  False\n",
      "r =  -0.9208380365432397\n",
      "d =  False\n",
      "r =  -0.6352165012877085\n",
      "d =  False\n",
      "r =  -0.3366263714803635\n",
      "d =  False\n",
      "r =  -0.24083728004394428\n",
      "d =  False\n",
      "r =  -0.4748343863952039\n",
      "d =  False\n",
      "r =  -0.7516155605903755\n",
      "d =  False\n",
      "r =  -0.8336558939171195\n",
      "d =  False\n",
      "r =  -0.741436466530964\n",
      "d =  False\n",
      "r =  -0.4686512341599123\n",
      "d =  False\n",
      "r =  -0.3826837267644709\n",
      "d =  False\n",
      "r =  -0.636156611608013\n",
      "d =  False\n",
      "r =  -0.8166242711262243\n",
      "d =  False\n",
      "r =  -0.8141650513807157\n",
      "d =  False\n",
      "r =  -0.5563239068301067\n",
      "d =  False\n",
      "r =  -0.4293015612021714\n",
      "d =  False\n",
      "r =  -0.7868286239639066\n",
      "d =  False\n",
      "r =  -0.8406053990909234\n",
      "d =  False\n",
      "r =  -0.7551087011922459\n",
      "d =  False\n",
      "r =  -0.5210177502505313\n",
      "d =  False\n",
      "r =  -0.6699859806646333\n",
      "d =  False\n",
      "r =  -0.8692338870638329\n",
      "d =  False\n",
      "r =  -0.7960688783607663\n",
      "d =  False\n",
      "r =  -0.5213728068545042\n",
      "d =  False\n",
      "r =  -0.6703843953242156\n",
      "d =  False\n",
      "r =  -0.8270029855734321\n",
      "d =  False\n",
      "r =  -0.7507899918575738\n",
      "d =  False\n",
      "r =  -0.4774860153897196\n",
      "d =  False\n",
      "r =  -0.7480703449776083\n",
      "d =  False\n",
      "r =  -0.8167176798887675\n",
      "d =  False\n",
      "r =  -0.5755158640941576\n",
      "d =  False\n",
      "r =  -0.5244323370950528\n",
      "d =  False\n",
      "r =  -0.8911678015283343\n",
      "d =  False\n",
      "r =  -0.7386281545119056\n",
      "d =  False\n",
      "r =  -0.35452367455316514\n",
      "d =  False\n",
      "r =  -0.6521213405972637\n",
      "d =  False\n",
      "r =  -0.7295190846962132\n",
      "d =  False\n",
      "r =  -0.2744739352201874\n",
      "d =  True\n",
      "r =  -1.0581384409859025\n",
      "d =  False\n",
      "steps: :[===                                     ]8%\t23800/300000\tmain of rewards = -2.058138\ttime cost = 0:02:55.152631\tTrue time of r = 476r =  -1.6118481256975934\n",
      "d =  False\n",
      "r =  -1.7305329892775916\n",
      "d =  False\n",
      "r =  -1.5449973164284672\n",
      "d =  False\n",
      "r =  -1.3368844409124527\n",
      "d =  False\n",
      "r =  -1.4065526114120994\n",
      "d =  False\n",
      "r =  -1.3550167387340841\n",
      "d =  False\n",
      "r =  -1.2171151128274318\n",
      "d =  False\n",
      "r =  -1.1582321768618622\n",
      "d =  False\n",
      "r =  -1.26274093189261\n",
      "d =  False\n",
      "r =  -1.2931124764974147\n",
      "d =  False\n",
      "r =  -1.0541247593980667\n",
      "d =  False\n",
      "r =  -0.9528394729903196\n",
      "d =  False\n",
      "r =  -0.9362397282890691\n",
      "d =  False\n",
      "r =  -0.38382944757389476\n",
      "d =  False\n",
      "r =  -0.3870104661571684\n",
      "d =  False\n",
      "r =  -0.2959682228969742\n",
      "d =  False\n",
      "r =  -0.4526047724732403\n",
      "d =  False\n",
      "r =  -0.7911402262574332\n",
      "d =  False\n",
      "r =  -0.8186591458426503\n",
      "d =  False\n",
      "r =  -0.6661927648548382\n",
      "d =  False\n",
      "r =  -0.4129107428333607\n",
      "d =  False\n",
      "r =  -0.3675855531618585\n",
      "d =  False\n",
      "r =  -0.7040618406033852\n",
      "d =  False\n",
      "r =  -0.8801197290074956\n",
      "d =  False\n",
      "r =  -0.7910642391776809\n",
      "d =  False\n",
      "r =  -0.4923978048658354\n",
      "d =  False\n",
      "r =  -0.4262061455031783\n",
      "d =  False\n",
      "r =  -0.8229446848123526\n",
      "d =  False\n",
      "r =  -0.9151463126348167\n",
      "d =  False\n",
      "r =  -0.6888007651855863\n",
      "d =  False\n",
      "r =  -0.4620189599906306\n",
      "d =  False\n",
      "r =  -0.6789935917419654\n",
      "d =  False\n",
      "r =  -0.922439096735651\n",
      "d =  False\n",
      "r =  -0.797549381740002\n",
      "d =  False\n",
      "r =  -0.4903381418943544\n",
      "d =  False\n",
      "r =  -0.6696572860832182\n",
      "d =  False\n",
      "r =  -0.911087904175943\n",
      "d =  False\n",
      "r =  -0.7539061511213766\n",
      "d =  False\n",
      "r =  -0.4580634590650071\n",
      "d =  False\n",
      "r =  -0.7922988128515744\n",
      "d =  False\n",
      "r =  -0.8824411606451452\n",
      "d =  False\n",
      "r =  -0.6408524195189731\n",
      "d =  False\n",
      "r =  -0.4906518968587351\n",
      "d =  False\n",
      "r =  -0.9353578347748301\n",
      "d =  False\n",
      "r =  -0.7486001083183171\n",
      "d =  False\n",
      "r =  -0.4519064606184314\n",
      "d =  False\n",
      "r =  -0.7343475206004969\n",
      "d =  False\n",
      "r =  -0.8987574443008011\n",
      "d =  False\n",
      "r =  -0.45167973717901755\n",
      "d =  True\n",
      "r =  -0.7010550813982677\n",
      "d =  False\n",
      "steps: :[===                                     ]8%\t23850/300000\tmain of rewards = -1.701055\ttime cost = 0:02:55.518472\tTrue time of r = 477r =  -1.743465896334591\n",
      "d =  False\n",
      "r =  -1.969015770685125\n",
      "d =  False\n",
      "r =  -1.7904054506380018\n",
      "d =  False\n",
      "r =  -1.4161751220237393\n",
      "d =  False\n",
      "r =  -1.300269798016353\n",
      "d =  False\n",
      "r =  -1.3774180127771274\n",
      "d =  False\n",
      "r =  -1.3219903599101752\n",
      "d =  False\n",
      "r =  -1.2554650131261267\n",
      "d =  False\n",
      "r =  -1.238181919850227\n",
      "d =  False\n",
      "r =  -1.2552752935290377\n",
      "d =  False\n",
      "r =  -1.1915656973507938\n",
      "d =  False\n",
      "r =  -1.1896102762841334\n",
      "d =  False\n",
      "r =  -0.8246279134636065\n",
      "d =  False\n",
      "r =  -0.8182881267897929\n",
      "d =  False\n",
      "r =  -0.6758809975381984\n",
      "d =  False\n",
      "r =  -0.633006259644368\n",
      "d =  False\n",
      "r =  -0.7480698907443837\n",
      "d =  False\n",
      "r =  -0.9447023817776872\n",
      "d =  False\n",
      "r =  -0.9398073173801677\n",
      "d =  False\n",
      "r =  -0.8193214139543274\n",
      "d =  False\n",
      "r =  -0.6582072797105776\n",
      "d =  False\n",
      "r =  -0.6943518623652163\n",
      "d =  False\n",
      "r =  -0.9256828820488358\n",
      "d =  False\n",
      "r =  -1.039487695660686\n",
      "d =  False\n",
      "r =  -0.8289679198888891\n",
      "d =  False\n",
      "r =  -0.6866749104676112\n",
      "d =  False\n",
      "r =  -0.8506008000674353\n",
      "d =  False\n",
      "r =  -1.094598635006093\n",
      "d =  False\n",
      "r =  -0.9585613107722453\n",
      "d =  False\n",
      "r =  -0.8187522629455273\n",
      "d =  False\n",
      "r =  -0.8786352485966908\n",
      "d =  False\n",
      "r =  -1.1756489646538189\n",
      "d =  False\n",
      "r =  -1.0729159519099787\n",
      "d =  False\n",
      "r =  -0.887087318749178\n",
      "d =  False\n",
      "r =  -1.0059533110826002\n",
      "d =  False\n",
      "r =  -1.2482561269596353\n",
      "d =  False\n",
      "r =  -0.9817666758604399\n",
      "d =  False\n",
      "r =  -0.921821483244437\n",
      "d =  False\n",
      "r =  -1.266910138657481\n",
      "d =  False\n",
      "r =  -1.2083578464650149\n",
      "d =  False\n",
      "r =  -0.9622166211244569\n",
      "d =  False\n",
      "r =  -1.262216755282911\n",
      "d =  False\n",
      "r =  -1.3220203159878947\n",
      "d =  False\n",
      "r =  -1.0194986855415675\n",
      "d =  False\n",
      "r =  -1.250298383768609\n",
      "d =  False\n",
      "r =  -1.3959286296361988\n",
      "d =  False\n",
      "r =  -1.1051731616443037\n",
      "d =  False\n",
      "r =  -1.314714204531378\n",
      "d =  False\n",
      "r =  -1.4324013688516504\n",
      "d =  True\n",
      "r =  -1.6294390298560917\n",
      "d =  False\n",
      "steps: :[===                                     ]8%\t23900/300000\tmain of rewards = -2.629439\ttime cost = 0:02:55.863454\tTrue time of r = 478"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18350/1991608276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"update_every\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0;31m# Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# detailed explanation). This converts batch-array of Transitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18350/302399041.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cosmic_rays_x_py38_env/lib/python3.8/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0mselected_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"batch_size\": 128,\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"hid\": 64,\n",
    "    \"l\": 3,\n",
    "    \"seed\": 0,\n",
    "    \"steps_per_epoch\": 3000,\n",
    "    \"epochs\": 100,\n",
    "    \"replay_size\": int(1e6),\n",
    "    \"gamma\": 0.99,\n",
    "    \"polyak\": 0.995,\n",
    "    \"pi_lr\": 1e-4,\n",
    "    \"q_lr\": 1e-4,\n",
    "    \"batch_size\": 500,\n",
    "    \"start_steps\": 10000, \n",
    "    \"update_after\": 1000,\n",
    "    \"update_every\": 50,\n",
    "    \"act_noise\": 0.01,\n",
    "    \"num_test_episodes\": 10,\n",
    "    \"max_ep_len\": 1000\n",
    "}\n",
    "\n",
    "nep_log[\"parameters\"] = params\n",
    "\n",
    "\n",
    "ac_kwargs=dict(hidden_sizes=[params[\"hid\"]]*params[\"l\"])\n",
    "\n",
    "torch.manual_seed(params[\"seed\"])\n",
    "np.random.seed(params[\"seed\"])\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "print('obs_dim = ', obs_dim)\n",
    "act_dim = env.action_space.shape[0]\n",
    "print('act_dim = ', act_dim)\n",
    "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "act_limit = env.action_space.high[0]\n",
    "print('act_limit = ', act_limit)\n",
    "# Create actor-critic module and target networks\n",
    "ac = MLPActorCritic(env.observation_space, env.action_space, **ac_kwargs)\n",
    "ac_targ = deepcopy(ac)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in ac_targ.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "memory = ReplayMemory(params[\"replay_size\"])\n",
    "\n",
    "\n",
    "# Set up function for computing DDPG Q-loss\n",
    "def compute_loss_q(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "    a = torch.cat(data.act).float()\n",
    "    r = torch.cat(data.rew).float()\n",
    "    o2 =torch.cat(data.next_obs).float()\n",
    "    d = torch.cat(data.done).float()\n",
    "\n",
    "    q = ac.q(o,a)\n",
    "\n",
    "\n",
    "    # Bellman backup for Q function\n",
    "    with torch.no_grad():\n",
    "        q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "        backup = r + params[\"gamma\"] * (1 - d) * q_pi_targ\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q = ((q - backup)**2).mean()\n",
    "\n",
    "    return loss_q\n",
    "\n",
    "# Set up function for computing DDPG pi loss\n",
    "def compute_loss_pi(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "\n",
    "    q_pi = ac.q(o, ac.pi(o))\n",
    "\n",
    "    return -q_pi.mean()\n",
    "\n",
    "pi_optimizer = RMSprop(ac.pi.parameters(), lr=params[\"pi_lr\"])\n",
    "q_optimizer = RMSprop(ac.q.parameters(), lr=params[\"q_lr\"])\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "\n",
    "\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q = compute_loss_q(data)\n",
    "\n",
    "    loss_q.backward()\n",
    "\n",
    "    q_optimizer.step()\n",
    "\n",
    "\n",
    "    # Freeze Q-network so you don't waste computational effort \n",
    "    # computing gradients for it during the policy learning step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Next run one gradient descent step for pi.\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(params[\"polyak\"])\n",
    "            p_targ.data.add_((1 - params[\"polyak\"]) * p.data)\n",
    "\n",
    "\n",
    "def get_action(o, noise_scale):\n",
    "    a = ac.act(torch.tensor(o, dtype=torch.float32, device=device))\n",
    "    a += noise_scale * torch.randn(act_dim).to(device)\n",
    "    return torch.clip(a, -act_limit, act_limit)\n",
    "\n",
    "def test_agent():\n",
    "    for j in range(params[\"num_test_episodes\"]):\n",
    "        o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "        while not(d or (ep_len == params[\"max_ep_len\"])):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "            # a = get_action(o, 0)\n",
    "            o, r, d, _ = test_env.step(a_cpu)\n",
    "            if d == True:\n",
    "                r += 10\n",
    "            else:\n",
    "                r += -1\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "        nep_log[\"test/reward\"].log(ep_ret/ep_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "total_steps = params[\"steps_per_epoch\"] * params[\"epochs\"]\n",
    "start_time = time.time()\n",
    "o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "o = torch.tensor([o], device=device)\n",
    "\n",
    "start_time_rec = datetime.datetime.now()\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "r_true = 0\n",
    "for t in range(total_steps):\n",
    "    \n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    if t > params[\"start_steps\"]:\n",
    "        a = get_action(o, params[\"act_noise\"])\n",
    "        # a_cpu = a\n",
    "        a_cpu = a.cpu().data.numpy()\n",
    "        \n",
    "    else:\n",
    "        a = env.action_space.sample()\n",
    "        a_cpu = a\n",
    "        a = torch.tensor([a], device=device)\n",
    "\n",
    "    # Step the env\n",
    "    o2, r, d, _ = env.step(a_cpu)\n",
    "    print('r = ', r)\n",
    "    print('d = ', d)\n",
    "    if d == True:\n",
    "        r += 10\n",
    "        r_true += 1\n",
    "    else:\n",
    "        r += -1\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "    ep_ret_main = ep_ret/ep_len\n",
    "\n",
    "\n",
    "    # Ignore the \"done\" signal if it comes from hitting the time\n",
    "    # horizon (that is, when it's an artificial terminal signal\n",
    "    # that isn't based on the agent's state)\n",
    "    d = False if ep_len==params[\"max_ep_len\"] else d\n",
    "\n",
    "    # a = torch.tensor([a], device=device)\n",
    "    o2 = torch.tensor([o2], device=device)\n",
    "    r = torch.tensor([r], device=device)\n",
    "    d = torch.tensor([d], device=device)\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    memory.push(o, a, r, o2, d)\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o=o2\n",
    "\n",
    "    # End of trajectory handling\n",
    "    if d or (ep_len == params[\"max_ep_len\"]):\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "    # Update handling\n",
    "    if t >= params[\"update_after\"] and t % params[\"update_every\"] == 0:\n",
    "        view_bar(\"steps: \", t, total_steps, 'main of rewards = ', float(ep_ret_main), start_time_rec, r_true)\n",
    "        \n",
    "        nep_log[\"train/reward\"].log(ep_ret_main)\n",
    "        for i in range(params[\"update_every\"]):\n",
    "\n",
    "            transitions = memory.sample(params[\"batch_size\"])\n",
    "            # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "            # detailed explanation). This converts batch-array of Transitions\n",
    "            # to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            # print('batch = ', str(batch))\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % params[\"steps_per_epoch\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35465c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_log.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca5cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12f8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a48a149bc7a8dee0435672efcae6f64e48d62311a35302b209b3ac517d7f9c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('cosmic_rays_x_py38_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

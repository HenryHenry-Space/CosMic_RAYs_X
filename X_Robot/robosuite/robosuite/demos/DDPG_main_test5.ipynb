{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "import gym\n",
    "from PIL import Image\n",
    "# from pyvirtualdisplay import Display\n",
    "# Display().start()\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop\n",
    "import gym\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robosuite as suite\n",
    "from robosuite.controllers import load_controller_config\n",
    "from robosuite.controllers.controller_factory import reset_controllers\n",
    "from robosuite.utils import observables\n",
    "from robosuite.utils.input_utils import *\n",
    "from robosuite.robots import Bimanual\n",
    "import imageio\n",
    "import numpy as np\n",
    "import robosuite.utils.macros as macros\n",
    "macros.IMAGE_CONVENTION = \"opencv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_log = neptune.init(\n",
    "    project=\"xhnfirst/DDPG-robosuite\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NTg5MDI2OS01MTVmLTQ2YjUtODA1Yy02ZWQyNDgxZDcwN2UifQ==\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'env_name': 'EElab_test5',\n",
    "    \"robots\": \"UR5e\"\n",
    "}\n",
    "controller_name = \"JOINT_VELOCITY\"\n",
    "options[\"controller_configs\"] = suite.load_controller_config(default_controller=controller_name)\n",
    "\n",
    "env = suite.make(\n",
    "    **options,\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=True,\n",
    "    ignore_done=True,\n",
    "    use_camera_obs=False,\n",
    "    gripper_types=None,\n",
    "    renderer = 'mujoco',\n",
    "\n",
    ")\n",
    "\n",
    "test_env = suite.make(\n",
    "    **options,\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=False,\n",
    "    ignore_done=True,\n",
    "    use_camera_obs=False,\n",
    "    gripper_types=None,\n",
    "    renderer = 'mujoco',\n",
    ")\n",
    "\n",
    "\n",
    "# video_env = suite.make(\n",
    "#     **options,\n",
    "#     gripper_types=None,\n",
    "#     has_renderer=False,\n",
    "#     has_offscreen_renderer=True,\n",
    "#     ignore_done=True,\n",
    "#     use_camera_obs=True,\n",
    "#     use_object_obs=True, \n",
    "#     camera_names='Labviewer',\n",
    "#     camera_heights=512,\n",
    "#     camera_widths=512,\n",
    "#     # control_freq=200,\n",
    "#     renderer = 'mujoco',\n",
    "# )\n",
    "\n",
    "frame = []\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device = ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = 35\n",
    "        act_dim = 6\n",
    "        act_limit = 1\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit).to(device)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('obs', 'act', 'rew', 'next_obs', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"hid\": 256,\n",
    "    \"l\": 3,\n",
    "    \"seed\": 0,\n",
    "    \"steps_per_epoch\": 3000,\n",
    "    \"steps_video\": 30000,\n",
    "    \"epochs\": 1000,\n",
    "    \"replay_size\": int(1e8),\n",
    "    \"gamma\": 0.99,\n",
    "    \"polyak\": 0.995,\n",
    "    \"pi_lr\": 1e-4,\n",
    "    \"q_lr\": 1e-4,\n",
    "    \"batch_size\": 1000,\n",
    "    \"start_steps\": 10000, \n",
    "    \"update_after\": 5000,\n",
    "    \"update_every\": 100,\n",
    "    \"act_noise\": 0.01,\n",
    "    \"num_test_episodes\": 5,\n",
    "    \"max_ep_len\": 300,\n",
    "    \"max_video_len\": 300,\n",
    "    \"save_model_len\": 10000,\n",
    "    # \"obs_dim\": 47,\n",
    "    # \"act_dim\": 7,\n",
    "    # \"act_limit\": 1\n",
    "}\n",
    "\n",
    "ac_kwargs=dict(hidden_sizes=[params[\"hid\"]]*params[\"l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_log[\"parameters\"] = params\n",
    "\n",
    "torch.manual_seed(params[\"seed\"])\n",
    "np.random.seed(params[\"seed\"])\n",
    "\n",
    "obs_dim = 35\n",
    "print('obs_dim = ', obs_dim)\n",
    "act_dim = 6\n",
    "print('act_dim = ', act_dim)\n",
    "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "act_limit = 1\n",
    "print('act_limit = ', act_limit)\n",
    "# Create actor-critic module and target networks\n",
    "ac = MLPActorCritic(**ac_kwargs)\n",
    "ac_targ = deepcopy(ac)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in ac_targ.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "memory = ReplayMemory(params[\"replay_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG Q-loss\n",
    "def compute_loss_q(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "    a = torch.cat(data.act).float()\n",
    "    r = torch.cat(data.rew).float()\n",
    "    o2 =torch.cat(data.next_obs).float()\n",
    "    d = torch.cat(data.done).float()\n",
    "\n",
    "    q = ac.q(o,a)\n",
    "\n",
    "\n",
    "    # Bellman backup for Q function\n",
    "    with torch.no_grad():\n",
    "        q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "        backup = r + params[\"gamma\"] * (1 - d) * q_pi_targ\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q = ((q - backup)**2).mean()\n",
    "\n",
    "    return loss_q\n",
    "\n",
    "# Set up function for computing DDPG pi loss\n",
    "def compute_loss_pi(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "\n",
    "    q_pi = ac.q(o, ac.pi(o))\n",
    "\n",
    "    return -q_pi.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_optimizer = RMSprop(ac.pi.parameters(), lr=params[\"pi_lr\"])\n",
    "q_optimizer = RMSprop(ac.q.parameters(), lr=params[\"q_lr\"])\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "\n",
    "\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q = compute_loss_q(data)\n",
    "\n",
    "    loss_q.backward()\n",
    "\n",
    "    q_optimizer.step()\n",
    "\n",
    "\n",
    "    # Freeze Q-network so you don't waste computational effort \n",
    "    # computing gradients for it during the policy learning step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Next run one gradient descent step for pi.\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(params[\"polyak\"])\n",
    "            p_targ.data.add_((1 - params[\"polyak\"]) * p.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_action(o, noise_scale):\n",
    "    a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "    a += noise_scale * torch.randn(act_dim).to(device)\n",
    "    return torch.clip(a, -act_limit, act_limit)\n",
    "\n",
    "def test_agent(epoch):\n",
    "    test_main = 0\n",
    "    test_step = 0\n",
    "    for j in range(params[\"num_test_episodes\"]):\n",
    "        obs, d, test_ep_ret, test_ep_len = test_env.reset(), False, 0, 0\n",
    "        o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "        o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "        while not(d or (test_ep_len == params[\"max_ep_len\"])):\n",
    "            a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "            obs, r, d, _ = test_env.step(a_cpu[0])\n",
    "            o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "            o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "            test_ep_ret += r\n",
    "            test_ep_len += 1\n",
    "        test_ep_main = test_ep_ret/test_ep_len\n",
    "        test_step +=1\n",
    "        test_main += test_ep_main\n",
    "    print('test_rew_main = ', float(test_main/test_step))\n",
    "    nep_log[\"test/reward\"].log(test_main/test_step)\n",
    "    \n",
    "# def video_agent(epoch):\n",
    "#     obs, d, test_ep_len = video_env.reset(), False, 0\n",
    "#     o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "#     o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "#     now = datetime.now()\n",
    "#     current_time = str(now.isoformat())\n",
    "#     writer = imageio.get_writer(\n",
    "#         \"/home/xhnfly/Cosmic_rays_X/X_Robot/robosuite/robosuite/demos/video/DDPG_re_touch_2_15_300/DDPG_UR5_%s_ep_%d.mp4\" % (current_time, epoch), fps=100)\n",
    "#     frame = obs[\"Labviewer_image\"]\n",
    "#     writer.append_data(frame)\n",
    "\n",
    "#     while not(d or (test_ep_len == params[\"max_video_len\"])):\n",
    "#         a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "#         obs, _, d, _ = video_env.step(a_cpu[0])\n",
    "#         o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "#         o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "#         frame = obs[\"Labviewer_image\"]\n",
    "#         writer.append_data(frame)\n",
    "#         test_ep_len += 1\n",
    "#     writer.close()\n",
    "#     nep_log['video'] = neptune.types.File('/home/xhnfly/Cosmic_rays_X/X_Robot/robosuite/robosuite/demos/video/DDPG_re_touch_2_15_300/DDPG_UR5_%s_ep_%d.mp4' % (current_time, epoch))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = {\n",
    "#     'robot0_joint_pos_cos': None,\n",
    "#     'robot0_joint_pos_sin': None,\n",
    "#     'robot0_joint_vel': None,\n",
    "#     'robot0_eef_pos': None,\n",
    "#     'robot0_eef_quat': None,\n",
    "#     'robot0_gripper_qpos': None,\n",
    "#     'robot0_gripper_qvel': None,\n",
    "#     'cubeA_pos': None,\n",
    "#     'cubeA_quat': None,\n",
    "#     'cubeB_pos': None,\n",
    "#     'cubeB_quat': None,\n",
    "#     'gripper_to_cubeA': None,\n",
    "#     'gripper_to_cubeB': None,\n",
    "#     'cubeA_to_cubeB': None,\n",
    "# }\n",
    "\n",
    "obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "\n",
    "# env.viewer.set_camera(camera_id=0)\n",
    "\n",
    "\n",
    "# Define neutral value\n",
    "neutral = np.zeros(7)\n",
    "\n",
    "# Keep track of done variable to know when to break loop\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "total_steps = params[\"steps_per_epoch\"] * params[\"epochs\"]\n",
    "start_time = time.time()\n",
    "\n",
    "o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "start_time_rec = datetime.now()\n",
    "r_true = 0\n",
    "total_main = 0\n",
    "ep_rew_main = 0\n",
    "reward_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop: collect experience in env and update/log each epoch\n",
    "low, high = env.action_spec\n",
    "\n",
    "for t in tqdm(range(total_steps)):\n",
    "    \n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    if t > params[\"start_steps\"]:\n",
    "        a = get_action(o, params[\"act_noise\"])      # Tensor\n",
    "    else:\n",
    "        a = torch.tensor([np.random.uniform(low, high)], dtype=torch.float32, device=device)\n",
    "        \n",
    "    a_cpu = a.cpu().data.numpy()\n",
    "    # Step the env\n",
    "    obs2, r, d, _ = env.step(a_cpu[0])\n",
    "    \n",
    "    o2 = list(obs2['robot0_proprio-state']) + list(obs2['object-state'])\n",
    "    # print('len(o2) = ', len(o2))\n",
    "\n",
    "    ep_len += 1\n",
    "    total_main += r\n",
    "\n",
    "\n",
    "    # Ignore the \"done\" signal if it comes from hitting the time\n",
    "    # horizon (that is, when it's an artificial terminal signal\n",
    "    # that isn't based on the agent's state)\n",
    "    d = False if ep_len==params[\"max_ep_len\"] else d\n",
    "\n",
    "    o2 = torch.tensor([o2], dtype=torch.float32, device=device)\n",
    "    r = torch.tensor([r], dtype=torch.float32, device=device)\n",
    "    d = torch.tensor([d], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    memory.push(o, a, r, o2, d)\n",
    "    nep_log[\"train/o\"].log(o)\n",
    "    nep_log[\"train/a\"].log(a)\n",
    "    nep_log[\"train/r\"].log(r)\n",
    "    nep_log[\"train/o2\"].log(o2)\n",
    "    nep_log[\"train/d\"].log(d)\n",
    "\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o=o2\n",
    "    ep_ret += r\n",
    "    \n",
    "    \n",
    "    # End of trajectory handling\n",
    "    if d or (ep_len == params[\"max_ep_len\"]):\n",
    "        ep_rew = ep_ret/ep_len\n",
    "        ep_rew_main += ep_rew\n",
    "        obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "        o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "    # Update handling\n",
    "    if t >= params[\"update_after\"] and t % params[\"update_every\"] == 0:\n",
    "        for i in range(params[\"update_every\"]):\n",
    "\n",
    "            transitions = memory.sample(params[\"batch_size\"])\n",
    "            # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "            # detailed explanation). This converts batch-array of Transitions\n",
    "            # to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % params[\"steps_per_epoch\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        train_reward = ep_rew_main/(params[\"steps_per_epoch\"]/params[\"max_ep_len\"])\n",
    "        nep_log[\"train/reward\"].log(train_reward)\n",
    "        print('train_rew_main = ', train_reward.cpu().data.numpy())\n",
    "        ep_rew_main = 0\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent(epoch)\n",
    "        \n",
    "\n",
    "    # if (t+1) % params[\"steps_video\"] == 0:\n",
    "    #     epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "    #     now = datetime.now()\n",
    "    #     current_time = str(now.isoformat())\n",
    "    #     print('current_time = ', current_time)\n",
    "    #     video_agent(epoch)\n",
    "    #     now = datetime.now()\n",
    "    #     current_time = str(now.isoformat())\n",
    "    #     print('current_time = ', current_time)\n",
    "\n",
    "    if (t+1) % params[\"save_model_len\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        now = datetime.now()\n",
    "        current_time = str(now.isoformat())\n",
    "        torch.save({\n",
    "                    'model of ac.q': ac.q.state_dict(),\n",
    "                    'model of ac.pi': ac.pi.state_dict(),\n",
    "                    'q_optimizer_state_dict': q_optimizer.state_dict(),\n",
    "                    'pi_optimizer_state_dict': pi_optimizer.state_dict(),\n",
    "                    \n",
    "                    }, \"model_nn/touch/model_nn_%s%d.pt\" % (current_time, epoch))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ac.q\n",
    "print(\"Model_q's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "model = ac.pi\n",
    "print(\"Model_pi's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pi_optimizer's state_dict:\")\n",
    "for var_name in pi_optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", pi_optimizer.state_dict()[var_name])\n",
    "\n",
    "print(\"q_optimizer's state_dict:\")\n",
    "for var_name in q_optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", q_optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = str(now.isoformat())\n",
    "\n",
    "\n",
    "\n",
    "torch.save({\n",
    "            'model of ac.q': ac.q.state_dict(),\n",
    "            'model of ac.pi': ac.pi.state_dict(),\n",
    "            'q_optimizer_state_dict': q_optimizer.state_dict(),\n",
    "            'pi_optimizer_state_dict': pi_optimizer.state_dict(),\n",
    "            \n",
    "            }, \"model_nn/model_nn_%s.pt\" % current_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_log.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a48a149bc7a8dee0435672efcae6f64e48d62311a35302b209b3ac517d7f9c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('cosmic_rays_x_py38_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
